大纲
全站定位与主线
人设
教育科技革新者（EdTech Innovator）：把真实课堂与产品场景里的问题抽象为研究议题，用严谨评估建立证据，再把证据沉入可规模化的学习系统（产学研一体化）。
全站统一主线
Measure → Intervene → Evidence → Scale
- Measure：把学习从“结果”扩展到“过程证据/可测指标”
- Intervene：把洞察转成课堂/产品可执行干预
- Evidence：实验/准实验/稳健性/公平性监控
- Scale：合规、资产化复用、跨场景迁移
顶部导航（5 页）
1. Home
2. Research Agenda（研究议程）
3. Selected Work（精选主卡项目）
4. Venture & Product Innovation（创业与产品创新｜重点加厚）
5. CV / Contact

1. Home


1.1 Hero（首屏）
一句话定位
EN：I build evidence-based learning systems—connecting measurement, intervention, and scalable educational technology.
关键词标签（3 个）
Educational Measurement · Learning Analytics · Evidence-based EdTech


1.2 解决的核心矛盾（3 条短句）

1. 教育技术常“看似有效”，但缺少可复现的证据标准与机制解释。
2. 学习过程不可见，导致干预靠经验，难以持续改进与迁移。
3. 创新若忽略实施约束与公平性，进入真实学校就容易失效。


1.3 Featured（首页置顶 3 张卡）
Tsinghua University（清华大学教育学院）｜MAIC：真实学校准实验 + 一线教研落地
HETERNAL EdTech（创业）｜从高校 LMS 到 K12/学前的证据驱动产品路线  
UN ESCAP（联合国亚太经社会）｜Vietnam SDG4 Evidence-to-Policy Pipeline  

每张置顶卡在首页只展示：Problem（一句）+ Approach（一句）+ Artifact（一项），点进去看完整主卡。


---

2. Research Agenda（研究议程）


这一页是“研究者身份页”，写问题，不写经历清单。

2.1 三个长期研究母题（直接对应你全部素材）

1. Actionable Measurement：如何把学习从分数扩展为过程证据，并做到有效、可解释、低负担实施？
2. Evidence Standards for EdTech & AI Classrooms：教育技术/AI课堂应采用怎样的实验与准实验框架，才能判断“真的有效、对谁有效”？
3. Scale without Drift：当创新进入真实学校约束（时间、资源、合规、差异），如何把干预设计成可复制的系统而不走样？


2.2 方法与视角（Methods & Lens）

- Measurement：IRT/BKT、任务与指标设计、校准与公平性检查
- Causal & Experimental：A/B、多臂、CUPED、异质性、ANCOVA/匹配/DiD 等真实世界识别
- Process data：课堂/学习轨迹数据、时间窗建模、置信度校准、分层监控
- Implementation：把研究输出做成学校可执行的 brief + annex + dashboard + SOP


2.3 你正在形成的“证据产品形态”（很加分）

- 可复现：数据字典/指标口径/实验协议/分析脚本
- 可执行：教师/教研端仪表盘与干预清单
- 可迁移：跨学校/跨学段的复用资产与评估基座


---

3. Selected Work（精选主卡项目｜按你要求调整）


这里我把重点从“ECNU 主卡”转走，把 THU 主卡上位；同时把 猿辅导 + 作业帮提升为主卡；并确保每张主卡都有明确机构名。

主卡统一模板（全站一致）


Context｜Problem｜Approach｜Evidence｜Artifacts｜My Role


---

主卡 A（政策与系统层｜置顶之一）


United Nations ESCAP｜Vietnam SDG4：Evidence-to-Policy Pipeline  

- Context：2015–2023 SDG 指标体系与区域差距
- Artifacts（建议展示）：one-page policy brief + technical annex、GIS dashboard、可复现脚本


---

主卡 B（高校产品层｜HETERNAL 线路 1）


HETERNAL × The University of Sydney｜Canvas LMS 智能增益（通知—作业—互动—诚信）

- 你提供的创业第一阶段素材为主（系统改造、可追溯数据、RAG/提示工程可复现、合规移交）
- Artifacts：插件 demo、数据架构图、合规/融合流程摘要


---

主卡 C（学校真实世界研究层｜置顶之一）


Tsinghua University（清华大学教育学院｜未来与创新课题组）｜MAIC：真实学校准实验 + 补偿效应机制 + 课堂落地

- 用你“吉林省实验学校学考复习”那段（准实验结构、量表信效度、ANCOVA、公平性结论、质性互证、产品迭代假设）作为核心
- Artifacts：研究设计 SOP（可复用）、分析框架图（宏观增值—异质性—公平性校正）、课堂落地手册（教研侧）


---

主卡 D（课堂过程数据与教学决策层｜HETERNAL 线路 2，补齐机构）


HETERNAL × Tsinghua University × 海淀区合作试点｜课堂多模态学习分析 × 教育大模型（教师可执行微策略）

- 这是你第二阶段最强的“过程数据→教学动作”闭环
- Artifacts：采集与对齐规范、信号处理与特征工程说明、模型校准/公平性审计页、教师轻量仪表盘与策略库


---

主卡 E（产业数据科学与评估层｜你要求必须主卡）


Yuanfudao（猿辅导）｜Data Science Intern：指标体系 × IRT/BKT × 实验评估  

- 你在简历里最硬的三件套：IRT/BKT、CUPED+因果森林+功效分析、落到生产看板与个性化复习  
- Artifacts：KPI tree、实验协议与 guardrail、个性化复习生成逻辑


---

主卡 F（课程与评价一体化｜你要求必须主卡）


Zuoyebang（作业帮）｜Course Design Intern：backward design × IRT/BKT × 公平性与DiD评估  

- 这张卡对“产线化课程设计 + 评估”非常有代表性
- Artifacts：coverage matrix、error taxonomy、题项校准与 DIF 抽检说明、家校卡片样例


---

主卡 G（Pre-K 数学产品化闭环｜建议保留为主卡，产品味最强）


TAL（好未来）｜Product Design Intern：Pre-K–G3 数学 learning progression → 可测指标 → 校准与公平性  

- 你在 CV 里有明确：标准→指标→自动化标注→看板→IRT/BKT 校准与公平性验证  
- Artifacts：课程蓝图、题项 rubric、讲解/纠错模板、实验与周报模板


---

短卡（2 张，一行/两行，不抢主线但保留硬度）

1. East China Normal University（华东师大）｜InnoSpark 教育大模型数据治理与评测闭环（不置顶、不当重点，但保留技术硬度）  
2. UNV（联合国志愿者）｜越南教育干预评估：DiD + HLM + policy brief  


DRR 那段我仍建议：只放 CV，不进网站，以保持“纯教育叙事”。


---

4. Venture & Product Innovation（创业与产品创新｜按你要求加厚）


这一页的目标：把你从“项目集合”升级为“教育科技革新者 + 产品系统建造者”。

4.1 HETERNAL 产品路线图（3 阶段，讲“产品—证据—规模化”）

- Phase 1｜Higher Ed（USYD / Canvas）：LMS 智能增益、可追溯数据、可复现 RAG/提示工程、合规移交
- Phase 2｜K12（与清华/海淀合作）：课堂过程数据 → 教师决策支持 → 低负担仪表盘与微策略
- Phase 3｜Early Years（3–6）：测评与训练闭环（强调测量学内控与可实施性）；你简历里对 EF 测量与信度/MDC 已有硬支撑  


4.2 你的“产品创新方法论”（建议做成 4 个模块，读起来像研究者在做产品）

1. Research-to-Product Translation：从一线问题 → 可测指标 → 干预假设 → MVP
2. Instrumentation-first：先定义事件模型/数据字典/指标口径，再做功能
3. Evidence Gate：关键改动必须过实验/准实验门槛（含 guardrails）
4. Implementation & Compliance by Design：知情同意、隐私评估、低带宽/低成本方案、学校侧 SOP


4.3 产品“可交付资产”（这部分建议写得非常具体）

- Data layer：data contract、事件模型、指标口径手册、血缘与回放
- Evaluation layer：实验协议、功效分析模板、稳健性/公平性审计清单
- Decision layer：教师仪表盘（趋势/阈值触发）、策略库（可执行、可审看）
- Deployment layer：合规包、培训手册、版本迭代与回滚机制


4.4 为什么这就是“产学研一体化”

- 学术侧：提出研究问题与识别策略
- 产业侧：把证据做成产品与运营闭环（你在猿辅导的评估与落地就能支撑这一点）  
- 公共/系统侧：把证据转译为可执行政策与资源配置（UN ESCAP）  


---

5. CV / Contact


5.1 CV

- 放 PDF 下载 + 1 屏摘要
- 摘要建议只保留与网站主线一致的条目：UN ESCAP、HETERNAL、猿辅导、作业帮、好未来、清华


5.2 Contact

- Email / LinkedIn / GitHub（可选）
- CTA 一句：欢迎围绕“学习测量、证据评估、AI课堂与教育系统落地”交流合作


---


主体内容

---

Site-wide components


Brand name + tagline


Name: Caesar Su
Tagline (short): Evidence-based EdTech Innovation
Tagline (long): Building measurable, interpretable, and scalable learning systems—bridging research, product, and real classrooms.

Top navigation (5 items)

- Home
- Research Agenda
- Selected Work
- Venture & Product Innovation
- CV / Contact


Global CTA button text

- Primary CTA: “View Selected Work”
- Secondary CTA: “Get in Touch”


Footer microcopy


Footer line: © Caesar Su — Evidence-based EdTech Innovation
Footer links: Email · LinkedIn · GitHub · CV PDF

SEO meta (copy/paste)


Meta title: Caesar Su | Evidence-based EdTech Innovator
Meta description: I build evidence-based learning systems—connecting educational measurement, learning analytics, and scalable EdTech. Work spans UN SDG4 evaluation, school-based quasi-experiments, and product innovation from LMS to AI classrooms.


---

Page 1 — Home


Hero section


Headline:
Building evidence-based learning systems—from measurement to scalable EdTech.

Subheadline (2 lines):
I work at the intersection of educational measurement, learning analytics, and product innovation.
My focus is translating rigorous evaluation into tools and systems that improve learning in real classrooms—at scale.

Tag chips:
Educational Measurement · Learning Analytics · Evidence-based EdTech · Research–Practice Translation · Implementation at Scale

Primary CTA button: View Selected Work
Secondary CTA button: Venture & Product Innovation

Hero supporting line (optional):
From UN SDG4 evidence-to-policy pipelines, to school-based quasi-experiments, to LMS and AI classroom deployments.

Hero image suggestion

- Option A (recommended): A clean, abstract background (AI-generated) + 1–2 real photos in a small collage (you in a school setting, workshop, or classroom observation).
- Alt text: “Abstract visualization of learning data and classroom signals.”


---

2. “What I’m solving” (3 bullets)


Section title: What I’m solving

Bullet 1 — Evidence gap:
EdTech often “looks effective,” but lacks reproducible evidence standards and mechanism-level insight.

Bullet 2 — Invisible learning process:
When learning processes are not measurable, interventions rely on intuition and cannot reliably improve over time.

Bullet 3 — Scale vs. reality:
Innovation fails in real schools when implementation constraints and fairness are not designed in from day one.

---

3. Your signature approach (Measure → Intervene → Evidence → Scale)


Section title: My approach

Measure
Define actionable learning indicators beyond test scores (e.g., calibrated items, process signals, learning readiness and self-regulation measures).

Intervene
Translate measurements into interventions that are feasible for teachers, students, and schools—micro-strategies, adaptive review, and course design improvements.

Evidence
Use experiments and quasi-experiments, robustness checks, and subgroup analysis to identify what works, for whom, and under what constraints.

Scale
Turn evidence into deployable systems: data contracts, evaluation gates, dashboards, SOPs, compliance-by-design, and reusable assets.

---

4. Featured work (ONLY 3 cards, as you wanted)


Section title: Featured

Featured Card 1 (UN ESCAP)


United Nations ESCAP — Vietnam SDG4: Evidence-to-Policy Pipeline
Building an end-to-end evaluation workflow that converts SDG data into actionable resource allocation and implementation playbooks for equity and learning quality.
Link text: Read the case

Featured Card 2 (HETERNAL)


HETERNAL — From LMS augmentation to AI classroom decision support
Founder-led product and research work across higher ed and K–12: instrumentation, evaluation, and deployable teacher-facing tools.
Link text: Explore the venture

Featured Card 3 (Tsinghua / MAIC)


Tsinghua University — MAIC: Real-school quasi-experiment & implementation
Designing evidence in high-stakes school settings, identifying compensatory effects, and translating findings into iteration hypotheses and classroom-ready routines.
Link text: View research + field implementation


---

5. Embedded media highlight (your UN presentation video)


Section title: Spotlight: SDG4 Evidence in Action

Copy:
I also share a long-form presentation that walks through the SDG4 evaluation logic, uncertainty boundaries, equity analyses, and the implementation translation layer.

Embed placeholder:
“Watch the full presentation (embedded)”

(Embed snippet provided at the end.)

---

6. Mini bio (tight, research-forward)


Section title: About

I’m an education technology innovator building evidence-based learning systems. My work spans international development evaluation (SDG4), school-based research and implementation, and product innovation across LMS augmentation, AI-assisted learning experiences, and teacher decision support. I’m especially interested in measurement, causal evaluation, process data, and scaling interventions without drift.

CTA: View CV / Contact


---

Page 2 — Research Agenda


1. Page header


Headline: Research Agenda
Subheadline: The long-term questions guiding my work across research, products, and real schools.


---

2. Core questions (3)


Q1 — Actionable measurement


How can we measure learning in ways that are valid, interpretable, and feasible for everyday classrooms?
I focus on measurement systems that are not just statistically sound, but decision-relevant—able to inform instruction, support, and iteration.

Q2 — Evidence standards for EdTech & AI classrooms


What should count as evidence for educational technologies—especially AI-mediated learning experiences?
I care about reproducibility, guardrails, robustness, and subgroup effects: not only “does it work,” but “for whom, when, and why.”

Q3 — Scaling without drift


How do we translate evidence into scalable systems without losing fidelity under real constraints?
Implementation constraints (time, hardware, connectivity, compliance, teacher workload) are not afterthoughts—they shape what is true and what is scalable.

---

3. Methods toolkit (organized, not a CV dump)


Section title: Methods & lenses

Measurement & calibration
Item response theory (IRT), knowledge tracing (BKT/DKT), validity/reliability checks, rater consistency, minimal detectable change, fairness audits (e.g., DIF).

Experimental & quasi-experimental evaluation
A/B testing, variance reduction (e.g., CUPED), heterogeneity analysis, ANCOVA and baseline adjustment, matched designs and difference-in-differences when randomization is constrained.

Process data & monitoring
Event logging and data contracts, time-window modeling, confidence calibration, subgroup monitoring, error taxonomies, and iterative evaluation pipelines.

Implementation translation
Turning evidence into operational artifacts: SOPs, dashboards, rubrics, briefs + technical annexes, and scalable training routines for schools and teams.

---

4. What I’m building next (forward-looking but concrete)


Section title: Near-term directions

- From outcome to process: expand beyond test and survey outcomes into richer process traces (interaction patterns, learning paths, teacher–AI–student dynamics).
- Compensatory effects at scale: identify when AI-enabled systems reduce gaps (vs. reinforcing them), and validate mechanisms in realistic settings.
- Evaluation infrastructure: build reusable “evidence gates” for product iteration—metrics, guardrails, and protocols that survive scaling.


---

Page 3 — Selected Work


1. Page header


Headline: Selected Work
Subheadline: Projects across policy evaluation, school-based research, and product innovation—each designed to produce evidence and deployable artifacts.

Filtering chips (optional UI):
Policy & Systems · School Research · Product & Platforms · Measurement & Evaluation

---

2. Main project cards (7 main cards)


Each card below includes the full copy you can paste into a “case study” page.
If you prefer shorter cards, keep “Card Summary” on the grid and move the rest into “Read more.”


---

Main Card A — United Nations ESCAP


Card title


United Nations ESCAP — Vietnam SDG4: Evidence-to-Policy Pipeline

Card summary (for the grid)


An end-to-end workflow connecting SDG indicators to equity diagnostics, uncertainty-aware reporting, and implementable intervention packages for local decision-makers.

Full case copy


Context
A six-month project evaluating Vietnam’s pathways toward SDG4 (quality education), with a central focus on equity and learning quality.

Problem
Education systems need decisions that are both data-grounded and implementable. The challenge is turning heterogeneous, noisy indicators into actionable priorities—without overstating what the data can prove.

Approach
I built a closed-loop pipeline: data harmonization → trend and breakpoint analysis → equity gap curves → scenario planning → implementation translation. The workflow explicitly separated what the data can answer from what it cannot, and packaged findings into both leadership-friendly and technical deliverables.

Evidence & outputs

- Clear recovery and divergence patterns across time, region, and subgroup slices
- Robustness checks and sensitivity analyses to stress-test conclusions
- A “brief + technical annex” deliverable system to enable fast decisions with reproducible detail
- GIS dashboards and visualization atlases for spatial targeting


Artifacts (what you can show on the website)

- SDG4 briefing (PDF) + technical annex (PDF)
- Interactive dashboard screenshots + short demo clip
- Methods note: breakpoint detection, decomposition, uncertainty boundaries
- “Implementation package” outline (intervention bundle + resource scenarios)


Suggested images

- Real: screenshots of your GIS/dashboard pages (high credibility)
- Real: stills from workshops/meetings (if available)
- AI: an abstract map/data background for the header (optional)


Links

- Embedded long-form presentation video: [YOUR VIDEO LINK HERE]
- Download: [Brief PDF] · [Technical Annex PDF] · [Dashboard Demo]


---

Main Card B — HETERNAL × University of Sydney (Canvas LMS)


Card title


HETERNAL × The University of Sydney — Canvas LMS Augmentation (Communication → Workflows → Integrity)

Card summary


A lightweight AI augmentation layer for Canvas that improves course communication and assignment workflows through traceable data pipelines and reproducible RAG + prompting modules.

Full case copy


Context
HETERNAL began in Sydney as a higher-ed LMS augmentation initiative focused on reducing friction across announcements, assignments, teacher–student interaction, and academic integrity.

Problem
Instructors lose time to repeated clarification cycles and inconsistent messaging. Meanwhile, many “AI add-ons” fail because outputs are not traceable, reproducible, or aligned with institutional constraints.

Approach
I built an architecture that connects registrar/course data to a traceable content store, then uses scenario-based prompting and retrieval-augmented generation so explanations, grading notes, and broadcast communication remain evidence-backed and repeatable.

Evidence & outputs

- Reduced communication overhead and improved readability/consistency of course messaging
- Demonstrated feasibility for institutional handoff into compliance and integration workflows
- Designed the system to be auditable, not just helpful


Artifacts

- System diagram: data sources → traceable store → RAG layer → teacher-facing outputs
- Prompting & retrieval design spec (reproducibility and boundaries)
- Sample UI mockups: announcement drafting, assignment guidance, integrity checks
- “Handoff pack” for institutional integration and compliance review


Suggested images

- Real: screenshots of plugin UI prototypes or demo screens
- AI: clean abstract “LMS workflow” hero background (optional)


Links

- Product demo: [DEMO LINK]
- One-page architecture: [PDF LINK]


---

Main Card C — Tsinghua University (MAIC Research)


Card title


Tsinghua University (School of Education) — MAIC: Real-world Quasi-Experiment & Compensatory Effects

Card summary


A high-stakes, school-based evaluation of AI-enabled classroom delivery with quasi-experimental design, mechanism analysis, and equity-oriented interpretation.

Full case copy


Context
In real schools, time is constrained and stakes are high. We evaluated MAIC-supported instruction in a setting where students must improve quickly and where traditional large-class teaching struggles to address heterogeneity.

Problem
Does an AI-empowered classroom model merely raise averages—or can it produce compensatory effects that help disadvantaged learners catch up?

Approach
I led the data-to-inference pipeline: consistent sample construction, measurement validation for constructs (e.g., motivation, self-regulated learning, self-efficacy, engagement), baseline adjustment, and subgroup heterogeneity analysis. I used ANCOVA-style adjustments to isolate performance changes beyond baseline differences, and connected quantitative findings with qualitative coding from interviews to test mechanism plausibility.

Evidence & outputs

- A coherent story across three layers: overall gains → who benefits most → equity interpretation under baseline control
- Mechanism hypotheses supported by qualitative themes (e.g., “safe zone” for asking questions, scaffolding effects)
- Practical limitations surfaced: risk of “autopilot learning,” experience boundaries, and implementation constraints


Artifacts

- Reusable evaluation SOP for quasi-experiments in schools
- Analysis framework diagram (macro gains → heterogeneity → fairness adjustment)
- Classroom implementation checklist (what must be true for the model to work)
- Iteration hypotheses that map directly to product/teaching design changes


Suggested images

- Real (high value): school photos (campus gate, classroom setup, teacher session) — blur faces
- Real: anonymized charts (pre/post distributions, subgroup plots)
- AI: none needed; authenticity matters most here


Links

- Research note: [PDF LINK]
- Implementation playbook: [PDF LINK]


---

Main Card D — HETERNAL × Tsinghua × Haidian (AI Classroom + Multimodal)


Card title


HETERNAL × Tsinghua University × Haidian Pilot — Multimodal Classroom Analytics + AI Micro-Strategies

Card summary


A classroom process-data system that transforms multimodal signals into calibrated, teacher-executable micro-strategies—designed for feasibility, interpretability, and fairness monitoring.

Full case copy


Context
In K–12 classrooms, learning-relevant signals are often invisible in the moment. We piloted a system that aligns classroom events with multimodal signals and turns model outputs into actionable teacher support.

Problem
How can we quantify engagement and cognitive load in a way that supports teachers—without overwhelming them with complexity or compromising interpretability and equity?

Approach
I built an end-to-end pipeline: consent and privacy protocol → synchronized data collection → signal preprocessing and quality control → time-window modeling with early/late fusion comparisons → confidence calibration and subgroup monitoring → teacher-facing dashboard focused on trends and threshold triggers. Strategy suggestions were generated with a constrained knowledge base and readability controls, with fast human review.

Evidence & outputs

- Robust segment-level prediction performance in cross-school validation (with calibrated confidence)  
- Improved teacher ratings of feasibility and usefulness of suggestions
- Reusable assets across data collection, processing, modeling, and classroom integration


Artifacts

- Data collection & alignment protocol
- Preprocessing and feature engineering spec
- Calibration + fairness monitoring checklist
- Teacher dashboard + strategy library (micro-interventions)


Suggested images

- Real: anonymized photos of sensor setup / classroom equipment (no faces)
- Real: dashboard screenshots
- AI: abstract “signal-to-decision” illustration (optional)


Links

- Demo dashboard: [LINK]
- Protocol pack: [PDF LINK]


---

Main Card E — Yuanfudao (Data Science Internship)


Card title


Yuanfudao — Learning Analytics & Causal Evaluation for Personalization

Card summary


Built a consistent measurement and evaluation stack—data contracts, KPI trees, IRT/BKT calibration, and experiment design—connecting learning outcomes to deployable decision support.

Full case copy


Context
A K–12 learning platform needs both operational health metrics and learning metrics, unified in a way that supports iteration decisions.

Problem
Without consistent instrumentation and evidence gates, it’s easy to optimize engagement metrics while failing to improve actual learning outcomes—or to misread correlation as causation.

Approach
I helped unify event schemas and metric definitions, then integrated measurement calibration (IRT/BKT) and evaluation workflows (A/B, variance reduction, heterogeneity checks). For non-randomizable cases, I used quasi-experimental adjustments to reduce confounding and report uncertainty.

Evidence & outputs

- Stable metric definitions with reproducible analysis scripts
- Personalized review generation grounded in calibrated task difficulty/knowledge states  
- Experiment protocols with guardrails to prevent harmful tradeoffs


Artifacts

- KPI tree and weekly business review dashboard design
- Experiment protocol templates + guardrail metrics
- Measurement calibration report (IRT/BKT + fairness checks)


Suggested images

- Real: anonymized KPI tree graphic (recreated cleanly)
- AI: none required


Links

- Sample KPI tree: [IMAGE/PDF]
- Evaluation protocol: [PDF]


---

Main Card F — Zuoyebang (Course Design Internship)


Card title


Zuoyebang — Backward Design + Measurement-Driven Course Production (Pre-A1/A1 English)

Card summary


Designed a coherent “goals–content–assessment” system with learning progression, item calibration, fairness checks, and production-ready quality control for large-scale delivery.

Full case copy


Context
Early English learning requires careful alignment of objectives, task design, and feedback—especially under low literacy constraints.

Problem
Content scale often introduces drift: misalignment with objectives, inconsistent difficulty, and uneven quality across cohorts.

Approach
I used backward design and constructive alignment to define can-do statements and observable indicators, then built learning progression coverage matrices, error taxonomies, and measurement/evaluation workflows (IRT calibration, DIF checks, cohort analytics). Key design choices were validated with experiments and robustness checks where feasible.

Evidence & outputs

- Improved alignment between tasks, explanations, and learning objectives
- Reusable QC rubrics and monitoring dashboards for sustained quality


Artifacts

- Coverage matrix + learning progression spec
- Item specs + rubric-based QC system
- Analytics dashboard design (cohort and item-level diagnostics)


Suggested images

- Real: a clean screenshot of a coverage matrix (anonymized)
- AI: optional minimal icon set (phonics/grammar/speaking)


Links

- Curriculum blueprint: [PDF]
- QC rubric: [PDF]


---

Main Card G — TAL (Product Design Internship)


Card title


TAL Education Group — Pre-K to G3 Math: Learning Progression → Interaction Design → Evidence Gates

Card summary


Built a full loop from learning standards to measurable indicators, child-friendly interactions, calibrated tasks (IRT/BKT), and experiment-validated iteration that can scale.

Full case copy


Context
Pre-K math learning must be developmentally appropriate, low-burden, and strongly scaffolded—while still measurable and improvable.

Problem
AI explanations and content generation can create inconsistency unless tightly constrained, evaluated, and quality-controlled.

Approach
I translated standards into measurable can-do indicators and interaction specs, then built instrumentation and evaluation layers: cohort KPIs, item calibration, and experimental validation for key instructional design choices. Outputs were packaged into production-ready templates and QC routines.

Evidence & outputs

- Higher completion and comprehension consistency through tighter scaffolding and QC
- Reusable “learn–practice–assess–review” production system


Artifacts

- Course blueprint + coverage matrix
- Interaction and explanation templates (controlled)
- Evaluation and reporting pack (dashboards + weekly review templates)


Suggested images

- Real: anonymized interaction screenshots (no child data)
- AI: simple illustration of manipulatives (optional)


Links

- Blueprint: [PDF]
- Template library (sample): [PDF]


---

Short cards (2) — keep them compact


Short Card 1 — East China Normal University (InnoSpark)


East China Normal University — Education LLMs: Data Governance, Alignment & Evaluation  
Industrial-scale pipeline for educational corpora governance and evaluation—designed to reduce hallucinations, improve teachability, and keep training reproducible.
Link: Read a short overview

Short Card 2 — United Nations Volunteers (Education evaluation)


United Nations Volunteers — Education Intervention Evaluation (Vietnam)
Applied difference-in-differences and multilevel models to estimate intervention effects across subgroups, translating findings into policy-facing briefs and uncertainty-aware visuals.
Link: View summary


---

Page 4 — Venture & Product Innovation (expanded, product-forward)


1. Page header


Headline: Venture & Product Innovation
Subheadline: How I translate research into deployable products—without losing evidence, interpretability, or implementation feasibility.


---

2. Venture overview (HETERNAL)


Section title: HETERNAL (Founder & Product Lead)

One-paragraph overview:
HETERNAL is my venture to build evidence-based learning infrastructure across education systems. The product strategy is not “AI everywhere,” but measurable interventions with explicit evidence gates, compliance-by-design, and reusable implementation assets.

---

3. Three-phase roadmap (high-impact section)


Phase 1 — Higher education (Sydney / Canvas LMS)


Headline: Phase 1 — LMS augmentation with traceable, reproducible AI
Copy:
We began by augmenting core LMS workflows—communication, assignment handling, teacher–student interactions, and integrity checks. The design principle was auditability: content and suggestions must be traceable to sources and reproducible across contexts.

What to show (assets):
Architecture diagram · UI mockups · integration/handoff note · before/after workflow illustration

Phase 2 — K–12 classrooms (Haidian × Tsinghua collaboration)


Headline: Phase 2 — Classroom process data → teacher decision support
Copy:
We built a pipeline that aligns classroom events with process signals and turns them into teacher-executable micro-strategies—delivered through a lightweight dashboard focused on trends and thresholds rather than raw complexity.

What to show (assets):
Consent & privacy workflow · signal processing schematic · dashboard screenshots · strategy library sample page

Phase 3 — Early years (3–6): learning readiness & self-regulation measurement loop


Headline: Phase 3 — Measurement-driven early support (measure → diagnose → practice → re-assess)
Copy:
We are developing a low-burden system that combines age-appropriate tasks, psychometric controls, and teacher/parent reporting that emphasizes feasibility and actionability. The goal is a practical loop: measure, diagnose, practice, and re-assess—embedded into everyday routines.

What to show (assets):
Task overview · reliability/consistency metrics page · teacher dashboard mock · parent report sample

---

4. Product principles (your “innovator philosophy”)


Section title: Product principles I won’t compromise on

1. Instrumentation-first — Define event models, data contracts, and metric dictionaries before shipping features.
2. Evidence gates — Key changes must pass evaluation thresholds with guardrails, not just engagement metrics.
3. Actionable outputs — The system should output “what to do next,” not just prediction scores.
4. Compliance-by-design — Consent, privacy, and minimization are built into the product, not added later.
5. Scale without drift — Every rollout includes SOPs, monitoring, and a plan for adaptation across contexts.


---

5. “What I can deliver” (very concrete, feels like a lab + product org)


Section title: Deliverables (research × product)

- Measurement layer: calibration reports, fairness checks, validity notes, task specs
- Evaluation layer: A/B protocol, quasi-experiment SOP, power templates, guardrail metrics
- Decision layer: dashboards, threshold triggers, strategy libraries, human-review workflows
- Implementation layer: training packs, school playbooks, compliance review materials, rollout checklists
- Reproducibility layer: versioned scripts, metric dictionaries, experiment registries, data documentation


---

Page 5 — CV / Contact


1. CV section


Headline: CV
Copy:
For a concise summary of my academic background, research, and professional work, download my CV below.  
Button: Download CV (PDF)
Optional link: View CV as a page

2. Contact section


Headline: Contact
Copy:
I’m always open to conversations about evidence-based EdTech, learning measurement, AI classroom evaluation, and translating research into scalable systems.
Email: caesar.su7@gmail.com  
Links: LinkedIn · GitHub · Google Scholar (optional)

Contact CTA buttons:

- “Email me”
- “Schedule a chat” (optional)


---

Asset checklist (what you should gather/create)


Real photos (recommended — authenticity)

1. School fieldwork photos (no identifiable student faces; blur if needed)
2. 
  - school gate / campus sign
  - classroom setup (wide angle, from back)
  - workshop / teacher session (faces blurred)
3. 
4. Dashboard screenshots (anonymized)
5. Artifacts screenshots
6. 
  - KPI tree diagram (re-drawn cleanly)
  - coverage matrix excerpt
  - protocol checklist excerpt
7. 
8. Your talk/presentation stills
9. 
  - a frame from your UN presentation video
10. 


AI-generated visuals (optional — for polish, not credibility)


Use AI images mainly for:
- hero background abstract
- section headers (subtle, consistent style)
- “signal-to-decision” conceptual illustration


Avoid AI images for:
- school settings (it can look staged)
- people portraits (less credible for admissions context)


---

AI image prompts (ready to use)


Hero background (abstract, clean)


Prompt:
“A minimal, modern abstract background representing learning analytics and educational measurement, with subtle layered nodes and time-series traces, white space, professional academic style, no text, no logos, high resolution, soft contrast.”

“Signal to decision” conceptual header


Prompt:
“A clean conceptual illustration of transforming multimodal classroom signals into actionable teacher decisions, showing abstract inputs flowing into a simple dashboard, minimal lines, modern academic diagram aesthetic, no text.”

SDG4 / policy analytics header


Prompt:
“An abstract map-like visualization with subtle gridlines and contour-like shapes, suggesting equity analysis and education indicators, minimal and professional, no country borders, no text.”

---

UN presentation video embed (paste into your page)


Replace YOUR_VIDEO_URL with your hosted link (YouTube/Vimeo/Cloudflare Stream, etc.):
<div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;border-radius:16px;">
  <iframe
    src="YOUR_VIDEO_URL"
    style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"
    allow="autoplay; fullscreen; picture-in-picture"
    allowfullscreen
    title="Vietnam SDG4 Evidence-to-Policy Presentation">
  </iframe>
</div>


---

Quick final note on visuals + privacy


Where minors or classrooms appear, use wide shots and blur faces. Admissions readers respond very positively to authentic field visuals, but privacy-safe presentation matters.

If you want, paste:
1. your video link, and
2. a folder/list of the photos you already have (even just filenames + what’s in them),
3. and I’ll map each image to the exact section/case card + write captions/alt-text to match the narrative.